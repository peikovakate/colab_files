{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparsenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peikovakate/colab_files/blob/master/sparsenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxMO9szX_guT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2\n",
        "# !pip install tensorflow==2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDMgM-Ee3RSc",
        "colab_type": "code",
        "outputId": "4a51053d-2081-4524-9a75-ee48cd323b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, ZeroPadding2D, Dense, Dropout, Activation, Convolution2D, Reshape\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw028-24zg8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras\n",
        "tensorflow.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5pBuiA-F3Ig",
        "colab_type": "text"
      },
      "source": [
        "## Project in Neural Networks for Data Science applications\n",
        "\n",
        "(Kateryna Peikova)\n",
        "\n",
        "# Sparsely Aggregated Convolutional Networks\n",
        "\n",
        "Link to the paper \\\n",
        "[1] Sparsely Aggregated Convolutional Networks, https://arxiv.org/abs/1801.05895\n",
        "\n",
        "\n",
        "**Aim of the project:**\n",
        "1. Implement DenseNet\n",
        "2. Implement Sparse aggregation from the paper inside the dense block\n",
        "3. Compare performance of two approaches (DenseNet and SparseNet) on the CIFAR10 dataset.\n",
        "\n",
        "**Summary of the SparseNet paper:**\n",
        "\n",
        "The authors came up with new method of output aggregation in deep CNN. Under sparse aggregation they mean skipping some connections inside of the dense block in DenseNet (or Residual blocks in ResNet).\n",
        "\n",
        "SparseNet will show better or same results with DenseNet that have approximately equal number of parameters. \n",
        "![Imagenet results](https://github.com/Lyken17/SparseNet/blob/master/images/imagenet_efficiency.png?raw=true)\n",
        "Results for ImageNet dataset. [1]\n",
        "\n",
        "However, it is important to notice that this tendency doesn't work on all datasets according to results presented in the paper. Moreover, sometimes authors compared models that were not close in the number of parameters. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfBNjDeVbAGq",
        "colab_type": "text"
      },
      "source": [
        "## Implementation\n",
        "[2] Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSFgeJXoa5NW",
        "colab_type": "text"
      },
      "source": [
        "#### Convolution block\n",
        "Convolution block is the main building block of CNN. \n",
        "According to the DenseNet paper [2], it consists of two **composite functions**:\n",
        "1. **Bottleneck layer**: BN -> ReLU -> Conv 1x1\n",
        "2. **Convolution layer**: BN -> ReLU -> Conv 3x3 with zero padding\n",
        "\n",
        "Additionally we insert conditional dropout layer after each convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmqwgztJ8s7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolution block: 1x1 convolution composite block + 3x3 convolution composite block\n",
        "# l block takes input depth (l-1)*k\n",
        "def conv_block(x, block_number, sub_block, nb_filter, dropout_rate=None):\n",
        "    conv_name_base = 'conv_b' + str(block_number) + '-conv_' + str(sub_block)\n",
        "    relu_name_base = 'conv_b' + str(block_number) + '-relu_' + str(sub_block)\n",
        "    dropout_name = \"conv_b\" +str(block_number)+ '-dropout_' + str(sub_block)\n",
        "\n",
        "    # 1st composite block (funciton)\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4  \n",
        "    # batch normalization to train faster\n",
        "    x = BatchNormalization(name=conv_name_base+'_x1_bn')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate, name=dropout_name+\"_x1\")(x)\n",
        "\n",
        "    # 2nd composite block (function)\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(name=conv_name_base+'_x2_bn')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Convolution2D(nb_filter, 3, 1, name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate, name=dropout_name+\"_x2\")(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQiR2AO1a4N0",
        "colab_type": "text"
      },
      "source": [
        "#### Dense Block\n",
        "\n",
        "Dense block is conceptually higher level in the DenseNet architecture and consists of N convolutional blocks implemented above. \n",
        "\n",
        "The difference between DenseNet and deep CNN:\n",
        "Convolutional layers in the dense block are concatenated before every convolutional block. \\\n",
        "As a result we have a very dense connections between convolutional blocks, where the name DenseNet comes from. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p14UKNb8fzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_block(x, block_number, n_layers, n_filters, growth_rate, dropout_rate=None):\n",
        "    # repeat and concatenate n convolutional blocks\n",
        "    \n",
        "    concat_feature_maps = x\n",
        "    for i in range(n_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feature_maps, block_number, branch, growth_rate, dropout_rate)\n",
        "        concat_feature_maps = tf.concat([concat_feature_maps, x], -1)\n",
        "\n",
        "    n_filters += growth_rate\n",
        "\n",
        "    return concat_feature_maps, n_filters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-BzYw9sa3it",
        "colab_type": "text"
      },
      "source": [
        "#### Sparse Block (Sparse Aggregation)\n",
        "Here we implement alternative version of the Dense block from SparseNet [1]. \n",
        "Idea: every convolutional block has sparse connections with previous blocks. Connections are made with exponential offset layers. So, in the dense block, layer $l$ has $\\log_2(l)$ connections with previous layers. \n",
        "\n",
        "10: 9, 8, 6, 2\n",
        "\n",
        "1, 2, 4, 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg19liIlIXvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sparse_layers(x_list):\n",
        "    count = len(x_list)\n",
        "    i = 1\n",
        "    inputs = []\n",
        "    while i <= count:\n",
        "        # insert into beggining\n",
        "        inputs.insert(0, x_list[count - i])\n",
        "        # inputs.append(x_list[count - i])\n",
        "        i *= 2\n",
        "    return inputs\n",
        "\n",
        "def sparse_block(x, block_number, n_layers, n_filters, growth_rate, dropout_rate=None):\n",
        "    x_list = [x]\n",
        "    channel_list = [n_filters]\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        sub_block = i+1\n",
        "        x = conv_block(x, block_number, sub_block, growth_rate, dropout_rate)\n",
        "        x_list.append(x)\n",
        "\n",
        "        fetch_outputs = get_sparse_layers(x_list)\n",
        "        x = tf.concat(fetch_outputs, axis=-1)\n",
        "\n",
        "        channel_list.append(growth_rate)\n",
        "\n",
        "    n_filters = sum(get_sparse_layers(channel_list))\n",
        "\n",
        "    return x, n_filters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biaHSCncasAP",
        "colab_type": "text"
      },
      "source": [
        "#### Transition block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4uJ7q6h8X26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers between dense blocks are called \"transition layers\"\n",
        "# to reduce depth of feature maps\n",
        "def transition_block(x, block_number, nb_filter, compression=1.0, dropout_rate=None):\n",
        "    conv_name = 'trans' + str(block_number) + \"-conv\"\n",
        "\n",
        "    # Convolutional layer\n",
        "    # Batch normalization to train faster\n",
        "    x = BatchNormalization(name=conv_name+'_batch_norm')(x)\n",
        "\n",
        "    # Activation\n",
        "    relu_name = 'trans' + str(block_number) + \"-relu\"\n",
        "    x = Activation('relu', name=relu_name)(x)\n",
        "\n",
        "    # Convolution\n",
        "    # bias is not needed because we use batch normalization before convolution\n",
        "    x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name, use_bias=False)(x)\n",
        "\n",
        "    # Dropout layer after each convolutional layer\n",
        "    dropout_name = \"trans\" +str(block_number)+ \"-dropout\"\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate, name=dropout_name)(x)\n",
        "\n",
        "    # Pooling layer\n",
        "    pool_name = 'trans' + str(block_number) +\"-pool\"\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name)(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS_VPLScbSVa",
        "colab_type": "text"
      },
      "source": [
        "#### Complete Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkaBzPMA_cZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def densenet_model(growth_rate=32, nb_filter=64, nb_layers = [6,12,24,16], reduction=0.0, \n",
        "             dropout_rate=0.0, classes=16, shape=(32, 32, 3), batch_size=128, sparsenet=False):\n",
        "  \n",
        "      # From architecture for ImageNet (Table 1 in the paper)\n",
        "    # nb_filter = 64\n",
        "    # nb_layers = [6,12,24,16] # For DenseNet-121\n",
        "    \n",
        "    if sparsenet:\n",
        "      compound_block = sparse_block\n",
        "    else:\n",
        "      compound_block = dense_block\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "    nb_dense_block = len(nb_layers)\n",
        "    \n",
        "    img_input = Input(shape=shape, name='data')\n",
        "\n",
        "    x = ZeroPadding2D((1, 1), name='conv1_zeropadding', batch_size=batch_size)(img_input)\n",
        "    x = Convolution2D(nb_filter, 3, 1, name='conv1', use_bias=False)(x)\n",
        "    \n",
        "    block_number = 0\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        block_number = block_idx+1\n",
        "        # Add dense block or sparse block\n",
        "        x, nb_filter = compound_block(x, block_number, nb_layers[block_idx], nb_filter, \n",
        "                                      growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, block_number, nb_filter, compression=compression, \n",
        "                             dropout_rate=dropout_rate)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "\n",
        "    final_block = block_number + 1\n",
        "    # add final dense/sparse block\n",
        "    # there is no transition here\n",
        "    x, nb_filter = compound_block(x, final_block, nb_layers[-1], nb_filter, \n",
        "                                  growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "    x = BatchNormalization(name='final_conv_bn')(x)\n",
        "    x = Activation('relu', name='final_relu')(x)\n",
        "    x = GlobalAveragePooling2D(name='final_pool')(x)\n",
        "    x = Dense(classes, name='final_dense')(x)\n",
        "    output = Activation('softmax', name='class_probabilities')(x)\n",
        "    \n",
        "    return Model(inputs=img_input, outputs=output, name = \"SparseNet\" if sparsenet else \"DenseNet\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLb1VCTBcFnG",
        "colab_type": "text"
      },
      "source": [
        "## Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO3sM3DGapyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "def millions(x, pos):\n",
        "    return '%1.1fM' % (x * 1e-6)\n",
        "\n",
        "formatter = FuncFormatter(millions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpZbXj7HzRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers = [10, 40, 100, 150]\n",
        "sparse_params = []\n",
        "dense_params = []\n",
        "for layers in n_layers:\n",
        "  model = densenet_model(sparsenet=True, dropout_rate=0.2, nb_layers=[layers], growth_rate=12, classes=10)\n",
        "  sparse_params.append(model.count_params())\n",
        "  model = densenet_model(sparsenet=False, dropout_rate=0.2, nb_layers=[layers], growth_rate=12, classes=10)\n",
        "  dense_params.append(model.count_params())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JisccrvcWuLD",
        "colab_type": "code",
        "outputId": "61254acf-ac00-47aa-dba9-5bdbe07fb922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((8,6))\n",
        "ax.yaxis.set_major_formatter(formatter)\n",
        "\n",
        "plt.plot(n_layers, dense_params, label=\"DenseNet\", marker=\"o\")\n",
        "plt.plot(n_layers, sparse_params, label=\"SparseNet\",  marker=\"o\")\n",
        "plt.ylabel(\"N of parameters\")\n",
        "plt.xlabel(\"N of layers\")\n",
        "plt.legend()\n",
        "plt.title(\"Parameters dependency on number of layers\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGDCAYAAADDONJAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJKomMPKbtVW",
        "colab_type": "text"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMSfTX3Hb4TK",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqiebv_vb9CF",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6v3kbMuhT4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(sparseNet=False, n_layers=[10], k=12):\n",
        "  model = densenet_model(sparsenet=sparseNet, dropout_rate=0.2, nb_layers=n_layers, growth_rate=k, classes=10, batch_size=128)\n",
        "  optimizer = Adam(lr=1e-3, amsgrad=True)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "  model.count_params()\n",
        "  return model, model.count_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U0uZ3T7VU7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    \"sparse\": {\n",
        "        \"layers\": [6, 10, 20],\n",
        "        \"growth_rate\": [12, 12, 12],\n",
        "        \"params\": [51346, 85082, 169794]\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"layers\": [5, 8, 13],\n",
        "        \"growth_rate\": [12, 12, 12],\n",
        "        \"params\": [53234, 91082, 166642]\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhkVfxKNSrPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_file_name(isSparseNet, n_layers, growth_rate):\n",
        "  model_name = \"SparseNet\" if isSparseNet else \"DenseNet\"\n",
        "  model_file = \"{}-{}-k{}.h5\".format(model_name, n_layers, growth_rate)\n",
        "  return model_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opF4VaTm5E2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, model_file, n_epochs=5):\n",
        "  (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "  trainX = trainX.astype('float32')\n",
        "  testX = testX.astype('float32')\n",
        "\n",
        "  cifar_mean = trainX.mean(axis=(0, 1, 2), keepdims=True)\n",
        "  cifar_std = trainX.std(axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "  trainX = (trainX - cifar_mean) / (cifar_std + 1e-8)\n",
        "  testX = (testX - cifar_mean) / (cifar_std + 1e-8)\n",
        "\n",
        "\n",
        "  generator = ImageDataGenerator(width_shift_range=5. / 32,\n",
        "                                height_shift_range=5. / 32,\n",
        "                                horizontal_flip=True)\n",
        "\n",
        "  generator.fit(trainX, seed=0)\n",
        "\n",
        "  # can save here model instance as well if save_weights_only=False\n",
        "  model_checkpoint = ModelCheckpoint(model_file, monitor=\"val_accuracy\", \n",
        "                                    save_best_only=True, verbose=1, \n",
        "                                    save_weights_only=False)\n",
        "\n",
        "  callbacks = [model_checkpoint]\n",
        "\n",
        "  batch_size=128\n",
        "\n",
        "  history = model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size),\n",
        "                      steps_per_epoch=len(trainX) // batch_size, \n",
        "                      epochs=n_epochs,\n",
        "                      callbacks=callbacks,\n",
        "                      validation_data=(testX, testY),\n",
        "                      validation_steps=testX.shape[0] // batch_size, \n",
        "                      verbose=1)\n",
        "  \n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hk1vAcP87Lx",
        "colab_type": "code",
        "outputId": "d9007091-8cf1-4f4c-d679-a236d1790fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sparse_history = {\n",
        "    \"models\": [],\n",
        "    \"history\": [],\n",
        "    \"names\": []\n",
        "}\n",
        "\n",
        "dense_history = {\n",
        "    \"models\": [],\n",
        "    \"history\": [],\n",
        "    \"names\": []\n",
        "}\n",
        "\n",
        "nb_epoch = 20\n",
        "for i in range(len(params['sparse']['layers'])):\n",
        "  # SparseNet\n",
        "  isSparseNet = True\n",
        "  l = params['sparse']['layers'][i]\n",
        "  k = params['sparse']['growth_rate'][i]\n",
        "  name = get_file_name(isSparseNet, l, k)\n",
        "  print(\"------------------------------------------------------\")\n",
        "  print(\"Training\", name)\n",
        "\n",
        "  model, _ = compile_model(isSparseNet, [l], k)\n",
        "  model, history = train(model, name, nb_epoch)\n",
        "\n",
        "  sparse_history[\"models\"].append(model)\n",
        "  sparse_history[\"history\"].append(history)\n",
        "  sparse_history[\"names\"].append(name)\n",
        "\n",
        "  # DenseNet\n",
        "  isSparseNet = False\n",
        "  l = params['dense']['layers'][i]\n",
        "  k = params['dense']['growth_rate'][i]\n",
        "  name = get_file_name(isSparseNet, l, k)\n",
        "  print(\"------------------------------------------------------\")\n",
        "  print(\"Training\", name)\n",
        "\n",
        "  model, _ = compile_model(isSparseNet, [l], k)\n",
        "  model, history = train(model, name, nb_epoch)\n",
        "\n",
        "  dense_history[\"models\"].append(model)\n",
        "  dense_history[\"history\"].append(history)\n",
        "  dense_history[\"names\"].append(name)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Training SparseNet-6-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7605 - accuracy: 0.3625\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.36980, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 1.7601 - accuracy: 0.3625 - val_loss: 1.7550 - val_accuracy: 0.3698\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4447 - accuracy: 0.4830\n",
            "Epoch 00002: val_accuracy improved from 0.36980 to 0.39800, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 96s 245ms/step - loss: 1.4443 - accuracy: 0.4831 - val_loss: 2.0645 - val_accuracy: 0.3980\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3261 - accuracy: 0.5238\n",
            "Epoch 00003: val_accuracy improved from 0.39800 to 0.50280, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 96s 246ms/step - loss: 1.3257 - accuracy: 0.5239 - val_loss: 1.4193 - val_accuracy: 0.5028\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2540 - accuracy: 0.5511\n",
            "Epoch 00004: val_accuracy did not improve from 0.50280\n",
            "390/390 [==============================] - 95s 243ms/step - loss: 1.2539 - accuracy: 0.5510 - val_loss: 2.2158 - val_accuracy: 0.4466\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2043 - accuracy: 0.5701\n",
            "Epoch 00005: val_accuracy improved from 0.50280 to 0.50550, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 95s 243ms/step - loss: 1.2045 - accuracy: 0.5701 - val_loss: 1.5746 - val_accuracy: 0.5055\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1665 - accuracy: 0.5847\n",
            "Epoch 00006: val_accuracy improved from 0.50550 to 0.53220, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 95s 244ms/step - loss: 1.1668 - accuracy: 0.5847 - val_loss: 1.4375 - val_accuracy: 0.5322\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1297 - accuracy: 0.5971\n",
            "Epoch 00007: val_accuracy did not improve from 0.53220\n",
            "390/390 [==============================] - 94s 241ms/step - loss: 1.1296 - accuracy: 0.5971 - val_loss: 2.1251 - val_accuracy: 0.4588\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1000 - accuracy: 0.6086\n",
            "Epoch 00008: val_accuracy improved from 0.53220 to 0.56870, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 1.1000 - accuracy: 0.6085 - val_loss: 1.3770 - val_accuracy: 0.5687\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0725 - accuracy: 0.6195\n",
            "Epoch 00009: val_accuracy did not improve from 0.56870\n",
            "390/390 [==============================] - 94s 241ms/step - loss: 1.0725 - accuracy: 0.6195 - val_loss: 1.4705 - val_accuracy: 0.5475\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0520 - accuracy: 0.6259\n",
            "Epoch 00010: val_accuracy improved from 0.56870 to 0.60200, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 94s 241ms/step - loss: 1.0518 - accuracy: 0.6260 - val_loss: 1.1998 - val_accuracy: 0.6020\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.6353\n",
            "Epoch 00011: val_accuracy did not improve from 0.60200\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 1.0303 - accuracy: 0.6349 - val_loss: 2.0762 - val_accuracy: 0.4747\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0098 - accuracy: 0.6420\n",
            "Epoch 00012: val_accuracy did not improve from 0.60200\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 1.0103 - accuracy: 0.6417 - val_loss: 1.4159 - val_accuracy: 0.5674\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9943 - accuracy: 0.6482\n",
            "Epoch 00013: val_accuracy improved from 0.60200 to 0.60330, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.9941 - accuracy: 0.6482 - val_loss: 1.2019 - val_accuracy: 0.6033\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9768 - accuracy: 0.6543\n",
            "Epoch 00014: val_accuracy did not improve from 0.60330\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.9769 - accuracy: 0.6542 - val_loss: 1.7059 - val_accuracy: 0.5171\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9589 - accuracy: 0.6606\n",
            "Epoch 00015: val_accuracy improved from 0.60330 to 0.60440, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.9590 - accuracy: 0.6606 - val_loss: 1.2847 - val_accuracy: 0.6044\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9482 - accuracy: 0.6632\n",
            "Epoch 00016: val_accuracy improved from 0.60440 to 0.61890, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 94s 241ms/step - loss: 0.9481 - accuracy: 0.6633 - val_loss: 1.2899 - val_accuracy: 0.6189\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9352 - accuracy: 0.6675\n",
            "Epoch 00017: val_accuracy did not improve from 0.61890\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.9349 - accuracy: 0.6676 - val_loss: 1.4688 - val_accuracy: 0.5908\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9265 - accuracy: 0.6716\n",
            "Epoch 00018: val_accuracy did not improve from 0.61890\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.9265 - accuracy: 0.6717 - val_loss: 1.3555 - val_accuracy: 0.6082\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9136 - accuracy: 0.6770\n",
            "Epoch 00019: val_accuracy improved from 0.61890 to 0.63470, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 94s 241ms/step - loss: 0.9136 - accuracy: 0.6770 - val_loss: 1.1275 - val_accuracy: 0.6347\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9008 - accuracy: 0.6809\n",
            "Epoch 00020: val_accuracy improved from 0.63470 to 0.63890, saving model to SparseNet-6-k12.h5\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.9010 - accuracy: 0.6808 - val_loss: 1.1540 - val_accuracy: 0.6389\n",
            "------------------------------------------------------\n",
            "Training DenseNet-5-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7878 - accuracy: 0.3477\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.33010, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 89s 228ms/step - loss: 1.7875 - accuracy: 0.3477 - val_loss: 1.8341 - val_accuracy: 0.3301\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4996 - accuracy: 0.4598\n",
            "Epoch 00002: val_accuracy improved from 0.33010 to 0.44760, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.4995 - accuracy: 0.4598 - val_loss: 1.6553 - val_accuracy: 0.4476\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3674 - accuracy: 0.5118\n",
            "Epoch 00003: val_accuracy improved from 0.44760 to 0.48250, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 227ms/step - loss: 1.3669 - accuracy: 0.5121 - val_loss: 1.5866 - val_accuracy: 0.4825\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2887 - accuracy: 0.5378\n",
            "Epoch 00004: val_accuracy improved from 0.48250 to 0.51620, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.2886 - accuracy: 0.5379 - val_loss: 1.3823 - val_accuracy: 0.5162\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2320 - accuracy: 0.5594\n",
            "Epoch 00005: val_accuracy improved from 0.51620 to 0.56460, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 1.2318 - accuracy: 0.5594 - val_loss: 1.2328 - val_accuracy: 0.5646\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1897 - accuracy: 0.5755\n",
            "Epoch 00006: val_accuracy did not improve from 0.56460\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.1894 - accuracy: 0.5757 - val_loss: 1.6398 - val_accuracy: 0.4887\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1555 - accuracy: 0.5881\n",
            "Epoch 00007: val_accuracy did not improve from 0.56460\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.1556 - accuracy: 0.5880 - val_loss: 1.3167 - val_accuracy: 0.5634\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1270 - accuracy: 0.5997\n",
            "Epoch 00008: val_accuracy did not improve from 0.56460\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 1.1269 - accuracy: 0.5998 - val_loss: 1.5604 - val_accuracy: 0.5203\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1021 - accuracy: 0.6068\n",
            "Epoch 00009: val_accuracy did not improve from 0.56460\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.1021 - accuracy: 0.6068 - val_loss: 1.4322 - val_accuracy: 0.5504\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0743 - accuracy: 0.6198\n",
            "Epoch 00010: val_accuracy improved from 0.56460 to 0.56820, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.0745 - accuracy: 0.6197 - val_loss: 1.4050 - val_accuracy: 0.5682\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0537 - accuracy: 0.6258\n",
            "Epoch 00011: val_accuracy improved from 0.56820 to 0.59760, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.0538 - accuracy: 0.6259 - val_loss: 1.2464 - val_accuracy: 0.5976\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.6341\n",
            "Epoch 00012: val_accuracy did not improve from 0.59760\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 1.0341 - accuracy: 0.6342 - val_loss: 1.7283 - val_accuracy: 0.5092\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0166 - accuracy: 0.6388\n",
            "Epoch 00013: val_accuracy improved from 0.59760 to 0.60320, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 1.0169 - accuracy: 0.6387 - val_loss: 1.2247 - val_accuracy: 0.6032\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9984 - accuracy: 0.6461\n",
            "Epoch 00014: val_accuracy did not improve from 0.60320\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 0.9987 - accuracy: 0.6459 - val_loss: 1.3508 - val_accuracy: 0.5888\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9857 - accuracy: 0.6507\n",
            "Epoch 00015: val_accuracy improved from 0.60320 to 0.61350, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.9856 - accuracy: 0.6508 - val_loss: 1.1875 - val_accuracy: 0.6135\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9680 - accuracy: 0.6597\n",
            "Epoch 00016: val_accuracy did not improve from 0.61350\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.9680 - accuracy: 0.6597 - val_loss: 1.2608 - val_accuracy: 0.5965\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.6607\n",
            "Epoch 00017: val_accuracy improved from 0.61350 to 0.62630, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 225ms/step - loss: 0.9519 - accuracy: 0.6608 - val_loss: 1.1718 - val_accuracy: 0.6263\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9445 - accuracy: 0.6676\n",
            "Epoch 00018: val_accuracy improved from 0.62630 to 0.62780, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.9444 - accuracy: 0.6676 - val_loss: 1.2282 - val_accuracy: 0.6278\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9305 - accuracy: 0.6736\n",
            "Epoch 00019: val_accuracy improved from 0.62780 to 0.65110, saving model to DenseNet-5-k12.h5\n",
            "390/390 [==============================] - 87s 224ms/step - loss: 0.9304 - accuracy: 0.6736 - val_loss: 1.1056 - val_accuracy: 0.6511\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.6736\n",
            "Epoch 00020: val_accuracy did not improve from 0.65110\n",
            "390/390 [==============================] - 88s 226ms/step - loss: 0.9210 - accuracy: 0.6737 - val_loss: 1.2641 - val_accuracy: 0.6120\n",
            "------------------------------------------------------\n",
            "Training SparseNet-10-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7087 - accuracy: 0.3779\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.37910, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 141s 361ms/step - loss: 1.7085 - accuracy: 0.3780 - val_loss: 1.7252 - val_accuracy: 0.3791\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3979 - accuracy: 0.4951\n",
            "Epoch 00002: val_accuracy improved from 0.37910 to 0.37960, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 140s 359ms/step - loss: 1.3976 - accuracy: 0.4952 - val_loss: 2.1493 - val_accuracy: 0.3796\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2667 - accuracy: 0.5453\n",
            "Epoch 00003: val_accuracy improved from 0.37960 to 0.52260, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 147s 376ms/step - loss: 1.2666 - accuracy: 0.5454 - val_loss: 1.5137 - val_accuracy: 0.5226\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1826 - accuracy: 0.5775\n",
            "Epoch 00004: val_accuracy did not improve from 0.52260\n",
            "390/390 [==============================] - 150s 384ms/step - loss: 1.1825 - accuracy: 0.5773 - val_loss: 1.5880 - val_accuracy: 0.5098\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1268 - accuracy: 0.5987\n",
            "Epoch 00005: val_accuracy improved from 0.52260 to 0.56370, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 154s 394ms/step - loss: 1.1271 - accuracy: 0.5984 - val_loss: 1.3888 - val_accuracy: 0.5637\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0819 - accuracy: 0.6169\n",
            "Epoch 00006: val_accuracy did not improve from 0.56370\n",
            "390/390 [==============================] - 148s 379ms/step - loss: 1.0817 - accuracy: 0.6171 - val_loss: 1.5260 - val_accuracy: 0.5381\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0366 - accuracy: 0.6331\n",
            "Epoch 00007: val_accuracy improved from 0.56370 to 0.60330, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 153s 393ms/step - loss: 1.0365 - accuracy: 0.6332 - val_loss: 1.3492 - val_accuracy: 0.6033\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0049 - accuracy: 0.6425\n",
            "Epoch 00008: val_accuracy did not improve from 0.60330\n",
            "390/390 [==============================] - 156s 400ms/step - loss: 1.0046 - accuracy: 0.6426 - val_loss: 1.3436 - val_accuracy: 0.5888\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9728 - accuracy: 0.6542\n",
            "Epoch 00009: val_accuracy did not improve from 0.60330\n",
            "390/390 [==============================] - 151s 387ms/step - loss: 0.9727 - accuracy: 0.6543 - val_loss: 1.7354 - val_accuracy: 0.5533\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9517 - accuracy: 0.6616\n",
            "Epoch 00010: val_accuracy improved from 0.60330 to 0.63760, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 155s 398ms/step - loss: 0.9517 - accuracy: 0.6616 - val_loss: 1.2005 - val_accuracy: 0.6376\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9316 - accuracy: 0.6697\n",
            "Epoch 00011: val_accuracy did not improve from 0.63760\n",
            "390/390 [==============================] - 152s 391ms/step - loss: 0.9315 - accuracy: 0.6698 - val_loss: 1.7298 - val_accuracy: 0.5800\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6793\n",
            "Epoch 00012: val_accuracy did not improve from 0.63760\n",
            "390/390 [==============================] - 155s 397ms/step - loss: 0.9092 - accuracy: 0.6793 - val_loss: 1.5806 - val_accuracy: 0.5786\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8926 - accuracy: 0.6844\n",
            "Epoch 00013: val_accuracy did not improve from 0.63760\n",
            "390/390 [==============================] - 149s 383ms/step - loss: 0.8925 - accuracy: 0.6843 - val_loss: 1.5922 - val_accuracy: 0.5939\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8798 - accuracy: 0.6900\n",
            "Epoch 00014: val_accuracy improved from 0.63760 to 0.66460, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 146s 373ms/step - loss: 0.8798 - accuracy: 0.6900 - val_loss: 1.2103 - val_accuracy: 0.6646\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8634 - accuracy: 0.6982\n",
            "Epoch 00015: val_accuracy did not improve from 0.66460\n",
            "390/390 [==============================] - 144s 369ms/step - loss: 0.8628 - accuracy: 0.6984 - val_loss: 1.7497 - val_accuracy: 0.5592\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8576 - accuracy: 0.6967\n",
            "Epoch 00016: val_accuracy did not improve from 0.66460\n",
            "390/390 [==============================] - 143s 367ms/step - loss: 0.8575 - accuracy: 0.6968 - val_loss: 1.6038 - val_accuracy: 0.6001\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8372 - accuracy: 0.7055\n",
            "Epoch 00017: val_accuracy did not improve from 0.66460\n",
            "390/390 [==============================] - 143s 366ms/step - loss: 0.8373 - accuracy: 0.7054 - val_loss: 1.1576 - val_accuracy: 0.6645\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8268 - accuracy: 0.7091\n",
            "Epoch 00018: val_accuracy did not improve from 0.66460\n",
            "390/390 [==============================] - 142s 365ms/step - loss: 0.8266 - accuracy: 0.7092 - val_loss: 1.3470 - val_accuracy: 0.6442\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8125 - accuracy: 0.7134\n",
            "Epoch 00019: val_accuracy did not improve from 0.66460\n",
            "390/390 [==============================] - 143s 367ms/step - loss: 0.8124 - accuracy: 0.7134 - val_loss: 2.2350 - val_accuracy: 0.5337\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8006 - accuracy: 0.7180\n",
            "Epoch 00020: val_accuracy improved from 0.66460 to 0.66560, saving model to SparseNet-10-k12.h5\n",
            "390/390 [==============================] - 142s 365ms/step - loss: 0.8006 - accuracy: 0.7182 - val_loss: 1.1885 - val_accuracy: 0.6656\n",
            "------------------------------------------------------\n",
            "Training DenseNet-8-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7043 - accuracy: 0.3746\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.37100, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 1.7034 - accuracy: 0.3750 - val_loss: 1.7504 - val_accuracy: 0.3710\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4158 - accuracy: 0.4876\n",
            "Epoch 00002: val_accuracy improved from 0.37100 to 0.45860, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 133s 341ms/step - loss: 1.4155 - accuracy: 0.4877 - val_loss: 1.5976 - val_accuracy: 0.4586\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2917 - accuracy: 0.5391\n",
            "Epoch 00003: val_accuracy improved from 0.45860 to 0.53830, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 1.2911 - accuracy: 0.5392 - val_loss: 1.3002 - val_accuracy: 0.5383\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2141 - accuracy: 0.5676\n",
            "Epoch 00004: val_accuracy did not improve from 0.53830\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 1.2139 - accuracy: 0.5678 - val_loss: 2.0513 - val_accuracy: 0.4453\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1556 - accuracy: 0.5858\n",
            "Epoch 00005: val_accuracy improved from 0.53830 to 0.56160, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 133s 340ms/step - loss: 1.1557 - accuracy: 0.5859 - val_loss: 1.3075 - val_accuracy: 0.5616\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1058 - accuracy: 0.6083\n",
            "Epoch 00006: val_accuracy did not improve from 0.56160\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 1.1056 - accuracy: 0.6082 - val_loss: 1.4447 - val_accuracy: 0.5596\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0716 - accuracy: 0.6204\n",
            "Epoch 00007: val_accuracy improved from 0.56160 to 0.60900, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 133s 340ms/step - loss: 1.0714 - accuracy: 0.6204 - val_loss: 1.1829 - val_accuracy: 0.6090\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0310 - accuracy: 0.6349\n",
            "Epoch 00008: val_accuracy did not improve from 0.60900\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 1.0308 - accuracy: 0.6350 - val_loss: 1.4423 - val_accuracy: 0.5458\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0035 - accuracy: 0.6433\n",
            "Epoch 00009: val_accuracy did not improve from 0.60900\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 1.0038 - accuracy: 0.6431 - val_loss: 1.3802 - val_accuracy: 0.5769\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9808 - accuracy: 0.6534\n",
            "Epoch 00010: val_accuracy improved from 0.60900 to 0.61350, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.9807 - accuracy: 0.6535 - val_loss: 1.2598 - val_accuracy: 0.6135\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9601 - accuracy: 0.6602\n",
            "Epoch 00011: val_accuracy did not improve from 0.61350\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.9598 - accuracy: 0.6603 - val_loss: 1.6245 - val_accuracy: 0.5485\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9332 - accuracy: 0.6713\n",
            "Epoch 00012: val_accuracy did not improve from 0.61350\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.9333 - accuracy: 0.6712 - val_loss: 1.3598 - val_accuracy: 0.6060\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9129 - accuracy: 0.6766\n",
            "Epoch 00013: val_accuracy did not improve from 0.61350\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.9131 - accuracy: 0.6766 - val_loss: 1.5627 - val_accuracy: 0.5754\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8979 - accuracy: 0.6806\n",
            "Epoch 00014: val_accuracy did not improve from 0.61350\n",
            "390/390 [==============================] - 132s 339ms/step - loss: 0.8983 - accuracy: 0.6806 - val_loss: 1.9764 - val_accuracy: 0.5421\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8833 - accuracy: 0.6884\n",
            "Epoch 00015: val_accuracy improved from 0.61350 to 0.65930, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.8835 - accuracy: 0.6882 - val_loss: 1.1068 - val_accuracy: 0.6593\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8677 - accuracy: 0.6945\n",
            "Epoch 00016: val_accuracy did not improve from 0.65930\n",
            "390/390 [==============================] - 132s 337ms/step - loss: 0.8676 - accuracy: 0.6945 - val_loss: 1.3978 - val_accuracy: 0.6042\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8563 - accuracy: 0.6993\n",
            "Epoch 00017: val_accuracy did not improve from 0.65930\n",
            "390/390 [==============================] - 131s 337ms/step - loss: 0.8561 - accuracy: 0.6994 - val_loss: 1.3305 - val_accuracy: 0.6279\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8448 - accuracy: 0.7037\n",
            "Epoch 00018: val_accuracy did not improve from 0.65930\n",
            "390/390 [==============================] - 131s 336ms/step - loss: 0.8446 - accuracy: 0.7038 - val_loss: 1.4004 - val_accuracy: 0.6309\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8285 - accuracy: 0.7105\n",
            "Epoch 00019: val_accuracy did not improve from 0.65930\n",
            "390/390 [==============================] - 131s 337ms/step - loss: 0.8285 - accuracy: 0.7105 - val_loss: 2.0266 - val_accuracy: 0.5513\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8195 - accuracy: 0.7117\n",
            "Epoch 00020: val_accuracy improved from 0.65930 to 0.66550, saving model to DenseNet-8-k12.h5\n",
            "390/390 [==============================] - 132s 338ms/step - loss: 0.8196 - accuracy: 0.7117 - val_loss: 1.1546 - val_accuracy: 0.6655\n",
            "------------------------------------------------------\n",
            "Training SparseNet-20-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.6859 - accuracy: 0.3791\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.36870, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 668ms/step - loss: 1.6854 - accuracy: 0.3793 - val_loss: 2.0288 - val_accuracy: 0.3687\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3368 - accuracy: 0.5203\n",
            "Epoch 00002: val_accuracy improved from 0.36870 to 0.41530, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 259s 664ms/step - loss: 1.3370 - accuracy: 0.5204 - val_loss: 1.7970 - val_accuracy: 0.4153\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1902 - accuracy: 0.5735\n",
            "Epoch 00003: val_accuracy improved from 0.41530 to 0.49020, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 1.1905 - accuracy: 0.5734 - val_loss: 1.7472 - val_accuracy: 0.4902\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0921 - accuracy: 0.6091\n",
            "Epoch 00004: val_accuracy improved from 0.49020 to 0.54660, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 667ms/step - loss: 1.0923 - accuracy: 0.6089 - val_loss: 1.5045 - val_accuracy: 0.5466\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0236 - accuracy: 0.6364\n",
            "Epoch 00005: val_accuracy did not improve from 0.54660\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 1.0234 - accuracy: 0.6365 - val_loss: 1.7751 - val_accuracy: 0.5443\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9772 - accuracy: 0.6527\n",
            "Epoch 00006: val_accuracy improved from 0.54660 to 0.61610, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 667ms/step - loss: 0.9775 - accuracy: 0.6526 - val_loss: 1.3061 - val_accuracy: 0.6161\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9315 - accuracy: 0.6719\n",
            "Epoch 00007: val_accuracy did not improve from 0.61610\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.9314 - accuracy: 0.6719 - val_loss: 1.4023 - val_accuracy: 0.6052\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8937 - accuracy: 0.6827\n",
            "Epoch 00008: val_accuracy did not improve from 0.61610\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.8940 - accuracy: 0.6827 - val_loss: 2.3807 - val_accuracy: 0.5327\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8705 - accuracy: 0.6919\n",
            "Epoch 00009: val_accuracy did not improve from 0.61610\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 0.8704 - accuracy: 0.6920 - val_loss: 1.6730 - val_accuracy: 0.5924\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8414 - accuracy: 0.7037\n",
            "Epoch 00010: val_accuracy did not improve from 0.61610\n",
            "390/390 [==============================] - 260s 667ms/step - loss: 0.8414 - accuracy: 0.7036 - val_loss: 2.1220 - val_accuracy: 0.5620\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8147 - accuracy: 0.7112\n",
            "Epoch 00011: val_accuracy improved from 0.61610 to 0.66960, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 0.8149 - accuracy: 0.7112 - val_loss: 1.1360 - val_accuracy: 0.6696\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7952 - accuracy: 0.7213\n",
            "Epoch 00012: val_accuracy improved from 0.66960 to 0.67320, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 0.7952 - accuracy: 0.7214 - val_loss: 1.2325 - val_accuracy: 0.6732\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7759 - accuracy: 0.7267\n",
            "Epoch 00013: val_accuracy did not improve from 0.67320\n",
            "390/390 [==============================] - 259s 664ms/step - loss: 0.7760 - accuracy: 0.7267 - val_loss: 1.3003 - val_accuracy: 0.6682\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7567 - accuracy: 0.7352\n",
            "Epoch 00014: val_accuracy improved from 0.67320 to 0.70460, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 667ms/step - loss: 0.7570 - accuracy: 0.7350 - val_loss: 1.0498 - val_accuracy: 0.7046\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7419 - accuracy: 0.7401\n",
            "Epoch 00015: val_accuracy did not improve from 0.70460\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.7420 - accuracy: 0.7400 - val_loss: 1.1953 - val_accuracy: 0.6942\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7212 - accuracy: 0.7468\n",
            "Epoch 00016: val_accuracy did not improve from 0.70460\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.7213 - accuracy: 0.7467 - val_loss: 1.2243 - val_accuracy: 0.6816\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7121 - accuracy: 0.7497\n",
            "Epoch 00017: val_accuracy did not improve from 0.70460\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.7123 - accuracy: 0.7497 - val_loss: 1.2613 - val_accuracy: 0.6821\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6985 - accuracy: 0.7556\n",
            "Epoch 00018: val_accuracy did not improve from 0.70460\n",
            "390/390 [==============================] - 259s 665ms/step - loss: 0.6990 - accuracy: 0.7554 - val_loss: 2.5175 - val_accuracy: 0.5524\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.7587\n",
            "Epoch 00019: val_accuracy improved from 0.70460 to 0.70550, saving model to SparseNet-20-k12.h5\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 0.6879 - accuracy: 0.7588 - val_loss: 1.1930 - val_accuracy: 0.7055\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6767 - accuracy: 0.7646\n",
            "Epoch 00020: val_accuracy did not improve from 0.70550\n",
            "390/390 [==============================] - 260s 665ms/step - loss: 0.6768 - accuracy: 0.7646 - val_loss: 1.4059 - val_accuracy: 0.6505\n",
            "------------------------------------------------------\n",
            "Training DenseNet-13-k12.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.6469 - accuracy: 0.3978\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.38570, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 213s 546ms/step - loss: 1.6464 - accuracy: 0.3980 - val_loss: 1.6505 - val_accuracy: 0.3857\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3474 - accuracy: 0.5126\n",
            "Epoch 00002: val_accuracy improved from 0.38570 to 0.50490, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 213s 545ms/step - loss: 1.3469 - accuracy: 0.5128 - val_loss: 1.4195 - val_accuracy: 0.5049\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2205 - accuracy: 0.5629\n",
            "Epoch 00003: val_accuracy did not improve from 0.50490\n",
            "390/390 [==============================] - 212s 543ms/step - loss: 1.2202 - accuracy: 0.5629 - val_loss: 2.0487 - val_accuracy: 0.4382\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1383 - accuracy: 0.5929\n",
            "Epoch 00004: val_accuracy improved from 0.50490 to 0.54580, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 213s 546ms/step - loss: 1.1381 - accuracy: 0.5929 - val_loss: 1.4825 - val_accuracy: 0.5458\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0735 - accuracy: 0.6178\n",
            "Epoch 00005: val_accuracy did not improve from 0.54580\n",
            "390/390 [==============================] - 213s 545ms/step - loss: 1.0732 - accuracy: 0.6178 - val_loss: 2.2695 - val_accuracy: 0.4559\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.6364\n",
            "Epoch 00006: val_accuracy did not improve from 0.54580\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 1.0250 - accuracy: 0.6365 - val_loss: 1.8446 - val_accuracy: 0.4915\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9807 - accuracy: 0.6551\n",
            "Epoch 00007: val_accuracy did not improve from 0.54580\n",
            "390/390 [==============================] - 213s 545ms/step - loss: 0.9804 - accuracy: 0.6553 - val_loss: 1.6406 - val_accuracy: 0.5412\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9419 - accuracy: 0.6689\n",
            "Epoch 00008: val_accuracy did not improve from 0.54580\n",
            "390/390 [==============================] - 213s 546ms/step - loss: 0.9418 - accuracy: 0.6689 - val_loss: 3.1636 - val_accuracy: 0.4149\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6786\n",
            "Epoch 00009: val_accuracy improved from 0.54580 to 0.61700, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 212s 545ms/step - loss: 0.9138 - accuracy: 0.6787 - val_loss: 1.2143 - val_accuracy: 0.6170\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8872 - accuracy: 0.6869\n",
            "Epoch 00010: val_accuracy did not improve from 0.61700\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.8871 - accuracy: 0.6870 - val_loss: 1.4963 - val_accuracy: 0.5874\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8563 - accuracy: 0.6970\n",
            "Epoch 00011: val_accuracy improved from 0.61700 to 0.62520, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 212s 545ms/step - loss: 0.8566 - accuracy: 0.6971 - val_loss: 1.3674 - val_accuracy: 0.6252\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8360 - accuracy: 0.7018\n",
            "Epoch 00012: val_accuracy did not improve from 0.62520\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.8362 - accuracy: 0.7016 - val_loss: 1.9604 - val_accuracy: 0.5278\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8195 - accuracy: 0.7105\n",
            "Epoch 00013: val_accuracy did not improve from 0.62520\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.8193 - accuracy: 0.7106 - val_loss: 2.0030 - val_accuracy: 0.5480\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7949 - accuracy: 0.7216\n",
            "Epoch 00014: val_accuracy did not improve from 0.62520\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.7955 - accuracy: 0.7213 - val_loss: 1.7036 - val_accuracy: 0.5969\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7741 - accuracy: 0.7286\n",
            "Epoch 00015: val_accuracy did not improve from 0.62520\n",
            "390/390 [==============================] - 213s 545ms/step - loss: 0.7739 - accuracy: 0.7287 - val_loss: 1.7862 - val_accuracy: 0.5596\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7648 - accuracy: 0.7309\n",
            "Epoch 00016: val_accuracy did not improve from 0.62520\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.7647 - accuracy: 0.7309 - val_loss: 2.4125 - val_accuracy: 0.5069\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7458 - accuracy: 0.7386\n",
            "Epoch 00017: val_accuracy improved from 0.62520 to 0.72930, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.7459 - accuracy: 0.7386 - val_loss: 0.9059 - val_accuracy: 0.7293\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7332 - accuracy: 0.7430\n",
            "Epoch 00018: val_accuracy did not improve from 0.72930\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.7336 - accuracy: 0.7428 - val_loss: 1.2716 - val_accuracy: 0.6338\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7154 - accuracy: 0.7512\n",
            "Epoch 00019: val_accuracy did not improve from 0.72930\n",
            "390/390 [==============================] - 212s 544ms/step - loss: 0.7155 - accuracy: 0.7511 - val_loss: 1.1376 - val_accuracy: 0.6894\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6995 - accuracy: 0.7554\n",
            "Epoch 00020: val_accuracy improved from 0.72930 to 0.74200, saving model to DenseNet-13-k12.h5\n",
            "390/390 [==============================] - 212s 545ms/step - loss: 0.6994 - accuracy: 0.7554 - val_loss: 0.8695 - val_accuracy: 0.7420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufxbRiR3VhIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    \"sparse\": {\n",
        "        \"layers\": [6, 10, 20, 16, 32],\n",
        "        \"growth_rate\": [12, 12, 12, 24, 24, 1027594],\n",
        "        \"params\": [51346, 85082, 169794, 490938]\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"layers\": [5, 8, 13, 12, 20],\n",
        "        \"growth_rate\": [12, 12, 12, 24, 24],\n",
        "        \"params\": [53234, 91082, 166642, 495306, 1015754]\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SfaPlI5VyKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6d0779e-641e-493b-b68f-66fb0539fa6a"
      },
      "source": [
        "nb_epoch = 20\n",
        "i=3\n",
        "# SparseNet\n",
        "isSparseNet = True\n",
        "l = params['sparse']['layers'][i]\n",
        "k = params['sparse']['growth_rate'][i]\n",
        "name = get_file_name(isSparseNet, l, k)\n",
        "print(\"------------------------------------------------------\")\n",
        "print(\"Training\", name)\n",
        "\n",
        "model, _ = compile_model(isSparseNet, [l], k)\n",
        "model, history = train(model, name, nb_epoch)\n",
        "\n",
        "sparse_history[\"models\"].append(model)\n",
        "sparse_history[\"history\"].append(history)\n",
        "sparse_history[\"names\"].append(name)\n",
        "\n",
        "# DenseNet\n",
        "isSparseNet = False\n",
        "l = params['dense']['layers'][i]\n",
        "k = params['dense']['growth_rate'][i]\n",
        "name = get_file_name(isSparseNet, l, k)\n",
        "print(\"------------------------------------------------------\")\n",
        "print(\"Training\", name)\n",
        "\n",
        "model, _ = compile_model(isSparseNet, [l], k)\n",
        "model, history = train(model, name, nb_epoch)\n",
        "\n",
        "dense_history[\"models\"].append(model)\n",
        "dense_history[\"history\"].append(history)\n",
        "dense_history[\"names\"].append(name)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Training SparseNet-16-k24.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.5532 - accuracy: 0.4298\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.41550, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 307s 786ms/step - loss: 1.5531 - accuracy: 0.4299 - val_loss: 1.9242 - val_accuracy: 0.4155\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2234 - accuracy: 0.5606\n",
            "Epoch 00002: val_accuracy did not improve from 0.41550\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 1.2232 - accuracy: 0.5607 - val_loss: 2.7094 - val_accuracy: 0.3980\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0588 - accuracy: 0.6225\n",
            "Epoch 00003: val_accuracy improved from 0.41550 to 0.42960, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 1.0586 - accuracy: 0.6225 - val_loss: 2.8872 - val_accuracy: 0.4296\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9623 - accuracy: 0.6595\n",
            "Epoch 00004: val_accuracy improved from 0.42960 to 0.60760, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 0.9620 - accuracy: 0.6596 - val_loss: 1.5662 - val_accuracy: 0.6076\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8910 - accuracy: 0.6850\n",
            "Epoch 00005: val_accuracy improved from 0.60760 to 0.61690, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 305s 781ms/step - loss: 0.8916 - accuracy: 0.6849 - val_loss: 1.4496 - val_accuracy: 0.6169\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8348 - accuracy: 0.7061\n",
            "Epoch 00006: val_accuracy did not improve from 0.61690\n",
            "390/390 [==============================] - 305s 781ms/step - loss: 0.8344 - accuracy: 0.7063 - val_loss: 2.7540 - val_accuracy: 0.5025\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7980 - accuracy: 0.7192\n",
            "Epoch 00007: val_accuracy did not improve from 0.61690\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 0.7982 - accuracy: 0.7192 - val_loss: 1.6599 - val_accuracy: 0.5953\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7539 - accuracy: 0.7355\n",
            "Epoch 00008: val_accuracy did not improve from 0.61690\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 0.7537 - accuracy: 0.7356 - val_loss: 1.7357 - val_accuracy: 0.6109\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7128 - accuracy: 0.7503\n",
            "Epoch 00009: val_accuracy improved from 0.61690 to 0.63580, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 0.7129 - accuracy: 0.7503 - val_loss: 1.5427 - val_accuracy: 0.6358\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.7588\n",
            "Epoch 00010: val_accuracy did not improve from 0.63580\n",
            "390/390 [==============================] - 304s 779ms/step - loss: 0.6890 - accuracy: 0.7588 - val_loss: 1.7033 - val_accuracy: 0.6187\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6524 - accuracy: 0.7717\n",
            "Epoch 00011: val_accuracy improved from 0.63580 to 0.70420, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 304s 780ms/step - loss: 0.6527 - accuracy: 0.7716 - val_loss: 1.1815 - val_accuracy: 0.7042\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6317 - accuracy: 0.7821\n",
            "Epoch 00012: val_accuracy did not improve from 0.70420\n",
            "390/390 [==============================] - 305s 781ms/step - loss: 0.6323 - accuracy: 0.7817 - val_loss: 2.5334 - val_accuracy: 0.5670\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7896\n",
            "Epoch 00013: val_accuracy did not improve from 0.70420\n",
            "390/390 [==============================] - 304s 779ms/step - loss: 0.6074 - accuracy: 0.7896 - val_loss: 1.2228 - val_accuracy: 0.6853\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5866 - accuracy: 0.7965\n",
            "Epoch 00014: val_accuracy did not improve from 0.70420\n",
            "390/390 [==============================] - 304s 780ms/step - loss: 0.5870 - accuracy: 0.7965 - val_loss: 1.6049 - val_accuracy: 0.6569\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.8038\n",
            "Epoch 00015: val_accuracy did not improve from 0.70420\n",
            "390/390 [==============================] - 304s 780ms/step - loss: 0.5688 - accuracy: 0.8038 - val_loss: 1.6422 - val_accuracy: 0.6617\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5453 - accuracy: 0.8123\n",
            "Epoch 00016: val_accuracy did not improve from 0.70420\n",
            "390/390 [==============================] - 304s 780ms/step - loss: 0.5455 - accuracy: 0.8123 - val_loss: 1.7065 - val_accuracy: 0.6606\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.8177\n",
            "Epoch 00017: val_accuracy improved from 0.70420 to 0.76460, saving model to SparseNet-16-k24.h5\n",
            "390/390 [==============================] - 305s 781ms/step - loss: 0.5271 - accuracy: 0.8177 - val_loss: 0.9410 - val_accuracy: 0.7646\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5166 - accuracy: 0.8209\n",
            "Epoch 00018: val_accuracy did not improve from 0.76460\n",
            "390/390 [==============================] - 304s 781ms/step - loss: 0.5165 - accuracy: 0.8210 - val_loss: 1.3959 - val_accuracy: 0.7051\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5065 - accuracy: 0.8250\n",
            "Epoch 00019: val_accuracy did not improve from 0.76460\n",
            "390/390 [==============================] - 305s 782ms/step - loss: 0.5068 - accuracy: 0.8250 - val_loss: 0.9618 - val_accuracy: 0.7644\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.8325\n",
            "Epoch 00020: val_accuracy did not improve from 0.76460\n",
            "390/390 [==============================] - 304s 781ms/step - loss: 0.4863 - accuracy: 0.8326 - val_loss: 1.2769 - val_accuracy: 0.7398\n",
            "------------------------------------------------------\n",
            "Training DenseNet-12-k24.h5\n",
            "Epoch 1/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.5577 - accuracy: 0.4265\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43930, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 282s 724ms/step - loss: 1.5572 - accuracy: 0.4269 - val_loss: 1.5991 - val_accuracy: 0.4393\n",
            "Epoch 2/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2710 - accuracy: 0.5436\n",
            "Epoch 00002: val_accuracy improved from 0.43930 to 0.45660, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 282s 723ms/step - loss: 1.2709 - accuracy: 0.5436 - val_loss: 1.6984 - val_accuracy: 0.4566\n",
            "Epoch 3/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1404 - accuracy: 0.5925\n",
            "Epoch 00003: val_accuracy improved from 0.45660 to 0.56230, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 1.1401 - accuracy: 0.5926 - val_loss: 1.3642 - val_accuracy: 0.5623\n",
            "Epoch 4/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0462 - accuracy: 0.6284\n",
            "Epoch 00004: val_accuracy did not improve from 0.56230\n",
            "390/390 [==============================] - 283s 725ms/step - loss: 1.0473 - accuracy: 0.6281 - val_loss: 1.9879 - val_accuracy: 0.5013\n",
            "Epoch 5/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9686 - accuracy: 0.6587\n",
            "Epoch 00005: val_accuracy did not improve from 0.56230\n",
            "390/390 [==============================] - 283s 725ms/step - loss: 0.9687 - accuracy: 0.6587 - val_loss: 1.8543 - val_accuracy: 0.5443\n",
            "Epoch 6/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.6758\n",
            "Epoch 00006: val_accuracy improved from 0.56230 to 0.59220, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.9209 - accuracy: 0.6757 - val_loss: 1.4552 - val_accuracy: 0.5922\n",
            "Epoch 7/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8720 - accuracy: 0.6937\n",
            "Epoch 00007: val_accuracy improved from 0.59220 to 0.63050, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.8720 - accuracy: 0.6937 - val_loss: 1.1814 - val_accuracy: 0.6305\n",
            "Epoch 8/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8264 - accuracy: 0.7099\n",
            "Epoch 00008: val_accuracy did not improve from 0.63050\n",
            "390/390 [==============================] - 282s 724ms/step - loss: 0.8265 - accuracy: 0.7099 - val_loss: 1.5601 - val_accuracy: 0.5894\n",
            "Epoch 9/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7945 - accuracy: 0.7236\n",
            "Epoch 00009: val_accuracy improved from 0.63050 to 0.63360, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.7945 - accuracy: 0.7236 - val_loss: 1.2606 - val_accuracy: 0.6336\n",
            "Epoch 10/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7628 - accuracy: 0.7340\n",
            "Epoch 00010: val_accuracy improved from 0.63360 to 0.63370, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.7630 - accuracy: 0.7339 - val_loss: 1.5149 - val_accuracy: 0.6337\n",
            "Epoch 11/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7309 - accuracy: 0.7457\n",
            "Epoch 00011: val_accuracy improved from 0.63370 to 0.63430, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 725ms/step - loss: 0.7308 - accuracy: 0.7455 - val_loss: 1.4348 - val_accuracy: 0.6343\n",
            "Epoch 12/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7066 - accuracy: 0.7519\n",
            "Epoch 00012: val_accuracy did not improve from 0.63430\n",
            "390/390 [==============================] - 283s 725ms/step - loss: 0.7067 - accuracy: 0.7518 - val_loss: 1.4232 - val_accuracy: 0.6263\n",
            "Epoch 13/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.7613\n",
            "Epoch 00013: val_accuracy improved from 0.63430 to 0.71740, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.6822 - accuracy: 0.7613 - val_loss: 1.0818 - val_accuracy: 0.7174\n",
            "Epoch 14/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6609 - accuracy: 0.7713\n",
            "Epoch 00014: val_accuracy did not improve from 0.71740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.6611 - accuracy: 0.7711 - val_loss: 1.4646 - val_accuracy: 0.6550\n",
            "Epoch 15/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6360 - accuracy: 0.7777\n",
            "Epoch 00015: val_accuracy did not improve from 0.71740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.6361 - accuracy: 0.7777 - val_loss: 1.7628 - val_accuracy: 0.6524\n",
            "Epoch 16/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6180 - accuracy: 0.7853\n",
            "Epoch 00016: val_accuracy improved from 0.71740 to 0.73740, saving model to DenseNet-12-k24.h5\n",
            "390/390 [==============================] - 284s 728ms/step - loss: 0.6180 - accuracy: 0.7853 - val_loss: 0.9713 - val_accuracy: 0.7374\n",
            "Epoch 17/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7916\n",
            "Epoch 00017: val_accuracy did not improve from 0.73740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.6001 - accuracy: 0.7915 - val_loss: 1.7134 - val_accuracy: 0.6611\n",
            "Epoch 18/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5845 - accuracy: 0.7977\n",
            "Epoch 00018: val_accuracy did not improve from 0.73740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.5844 - accuracy: 0.7979 - val_loss: 1.3417 - val_accuracy: 0.6815\n",
            "Epoch 19/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7999\n",
            "Epoch 00019: val_accuracy did not improve from 0.73740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.5740 - accuracy: 0.7998 - val_loss: 1.4335 - val_accuracy: 0.6850\n",
            "Epoch 20/20\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5596 - accuracy: 0.8064\n",
            "Epoch 00020: val_accuracy did not improve from 0.73740\n",
            "390/390 [==============================] - 283s 726ms/step - loss: 0.5592 - accuracy: 0.8065 - val_loss: 1.3671 - val_accuracy: 0.6929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpEcmG4CXMBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "sparse_val_error = [1 - max(history.history[\"val_accuracy\"]) for history in sparse_history[\"history\"]]\n",
        "dense_val_error = [1 - max(history.history[\"val_accuracy\"]) for history in dense_history[\"history\"]]\n",
        "\n",
        "with open('sparse_val_error.pkl', 'wb') as fp:\n",
        "    pickle.dump(sparse_val_error, fp)\n",
        "\n",
        "with open('dense_val_error.pkl', 'wb') as fp:\n",
        "    pickle.dump(dense_val_error, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx0Jm5Nkafdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "c9e9a079-dc33-4641-f10b-98e7fce4275d"
      },
      "source": [
        "sparse_val_error = [1 - max(history.history[\"val_accuracy\"]) for history in sparse_history[\"history\"]]\n",
        "dense_val_error = [1 - max(history.history[\"val_accuracy\"]) for history in dense_history[\"history\"]]\n",
        "\n",
        "dense_params = params[\"dense\"][\"params\"][:4]\n",
        "sparse_params = params[\"sparse\"][\"params\"][:4]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((8,6))\n",
        "ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "plt.plot(dense_params, dense_val_error, label=\"DenseNet\", marker=\"o\")\n",
        "plt.plot(sparse_params, sparse_val_error, label=\"SparseNet\",  marker=\"o\")\n",
        "\n",
        "\n",
        "plt.ylabel(\"Validation error\")\n",
        "plt.xlabel(\"N of parameters\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFzCAYAAADSXxtkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLypzQisAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}